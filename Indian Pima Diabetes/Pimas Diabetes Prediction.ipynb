{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets_228_482_diabetes.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Outcome\", axis=1).values\n",
    "y = data[\"Outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Tensors\n",
    "X_train=torch.FloatTensor(X_train)\n",
    "X_test=torch.FloatTensor(X_test)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "y_test=torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model with pytorch\n",
    "\n",
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self,input_features=8,hidden1=20,hidden2=20,output_features=2):\n",
    "        super().__init__()\n",
    "        self.f_connected1=nn.Linear(input_features,hidden1)\n",
    "        self.f_connected2=nn.Linear(hidden1,hidden2)   \n",
    "        self.out=nn.Linear(hidden2,output_features)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.f_connected1(x))\n",
    "        x=F.relu(self.f_connected2(x))\n",
    "        x=self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate the ANN_Model\n",
    "torch.manual_seed(20)\n",
    "model=ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ANN_Model(\n",
       "  (f_connected1): Linear(in_features=8, out_features=20, bias=True)\n",
       "  (f_connected2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (out): Linear(in_features=20, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backward propagation\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  1 and the loss : 3.457212209701538\n",
      "Epoch number:  2 and the loss : 1.7547036409378052\n",
      "Epoch number:  3 and the loss : 1.4100372791290283\n",
      "Epoch number:  4 and the loss : 0.9451971650123596\n",
      "Epoch number:  5 and the loss : 1.1574180126190186\n",
      "Epoch number:  6 and the loss : 0.8915755152702332\n",
      "Epoch number:  7 and the loss : 0.7283689975738525\n",
      "Epoch number:  8 and the loss : 0.8199499845504761\n",
      "Epoch number:  9 and the loss : 0.6772931814193726\n",
      "Epoch number:  10 and the loss : 0.6956121921539307\n",
      "Epoch number:  11 and the loss : 0.8019212484359741\n",
      "Epoch number:  12 and the loss : 0.7153996229171753\n",
      "Epoch number:  13 and the loss : 0.6444704532623291\n",
      "Epoch number:  14 and the loss : 0.6793992519378662\n",
      "Epoch number:  15 and the loss : 0.6693853139877319\n",
      "Epoch number:  16 and the loss : 0.6191177368164062\n",
      "Epoch number:  17 and the loss : 0.6354078650474548\n",
      "Epoch number:  18 and the loss : 0.6668112277984619\n",
      "Epoch number:  19 and the loss : 0.6471998691558838\n",
      "Epoch number:  20 and the loss : 0.6048517227172852\n",
      "Epoch number:  21 and the loss : 0.6090322732925415\n",
      "Epoch number:  22 and the loss : 0.6296099424362183\n",
      "Epoch number:  23 and the loss : 0.6203304529190063\n",
      "Epoch number:  24 and the loss : 0.6038042306900024\n",
      "Epoch number:  25 and the loss : 0.6073708534240723\n",
      "Epoch number:  26 and the loss : 0.6146319508552551\n",
      "Epoch number:  27 and the loss : 0.6013968586921692\n",
      "Epoch number:  28 and the loss : 0.5841547250747681\n",
      "Epoch number:  29 and the loss : 0.5858445763587952\n",
      "Epoch number:  30 and the loss : 0.592286229133606\n",
      "Epoch number:  31 and the loss : 0.5917770266532898\n",
      "Epoch number:  32 and the loss : 0.5871493816375732\n",
      "Epoch number:  33 and the loss : 0.5798207521438599\n",
      "Epoch number:  34 and the loss : 0.5764400959014893\n",
      "Epoch number:  35 and the loss : 0.575980544090271\n",
      "Epoch number:  36 and the loss : 0.5748282074928284\n",
      "Epoch number:  37 and the loss : 0.5752089023590088\n",
      "Epoch number:  38 and the loss : 0.5746603608131409\n",
      "Epoch number:  39 and the loss : 0.571656346321106\n",
      "Epoch number:  40 and the loss : 0.5688306093215942\n",
      "Epoch number:  41 and the loss : 0.5679707527160645\n",
      "Epoch number:  42 and the loss : 0.5667037963867188\n",
      "Epoch number:  43 and the loss : 0.5651770830154419\n",
      "Epoch number:  44 and the loss : 0.564317524433136\n",
      "Epoch number:  45 and the loss : 0.5630188584327698\n",
      "Epoch number:  46 and the loss : 0.5596032738685608\n",
      "Epoch number:  47 and the loss : 0.5561586618423462\n",
      "Epoch number:  48 and the loss : 0.5548204183578491\n",
      "Epoch number:  49 and the loss : 0.5543771386146545\n",
      "Epoch number:  50 and the loss : 0.5538845658302307\n",
      "Epoch number:  51 and the loss : 0.5529040098190308\n",
      "Epoch number:  52 and the loss : 0.551139235496521\n",
      "Epoch number:  53 and the loss : 0.5499550104141235\n",
      "Epoch number:  54 and the loss : 0.5491607785224915\n",
      "Epoch number:  55 and the loss : 0.5480620265007019\n",
      "Epoch number:  56 and the loss : 0.5463635325431824\n",
      "Epoch number:  57 and the loss : 0.5447407960891724\n",
      "Epoch number:  58 and the loss : 0.543453574180603\n",
      "Epoch number:  59 and the loss : 0.5429003834724426\n",
      "Epoch number:  60 and the loss : 0.5422551035881042\n",
      "Epoch number:  61 and the loss : 0.5410096645355225\n",
      "Epoch number:  62 and the loss : 0.5395323634147644\n",
      "Epoch number:  63 and the loss : 0.5384440422058105\n",
      "Epoch number:  64 and the loss : 0.5378201007843018\n",
      "Epoch number:  65 and the loss : 0.5369913578033447\n",
      "Epoch number:  66 and the loss : 0.5358226895332336\n",
      "Epoch number:  67 and the loss : 0.5347582697868347\n",
      "Epoch number:  68 and the loss : 0.5339264273643494\n",
      "Epoch number:  69 and the loss : 0.5331685543060303\n",
      "Epoch number:  70 and the loss : 0.5321905016899109\n",
      "Epoch number:  71 and the loss : 0.5310391783714294\n",
      "Epoch number:  72 and the loss : 0.5292231440544128\n",
      "Epoch number:  73 and the loss : 0.529935359954834\n",
      "Epoch number:  74 and the loss : 0.528609037399292\n",
      "Epoch number:  75 and the loss : 0.5279224514961243\n",
      "Epoch number:  76 and the loss : 0.5274975299835205\n",
      "Epoch number:  77 and the loss : 0.5261572599411011\n",
      "Epoch number:  78 and the loss : 0.5250259637832642\n",
      "Epoch number:  79 and the loss : 0.524620532989502\n",
      "Epoch number:  80 and the loss : 0.5231795907020569\n",
      "Epoch number:  81 and the loss : 0.522036075592041\n",
      "Epoch number:  82 and the loss : 0.521541178226471\n",
      "Epoch number:  83 and the loss : 0.5207496285438538\n",
      "Epoch number:  84 and the loss : 0.5193599462509155\n",
      "Epoch number:  85 and the loss : 0.5185878276824951\n",
      "Epoch number:  86 and the loss : 0.5177826285362244\n",
      "Epoch number:  87 and the loss : 0.5171576142311096\n",
      "Epoch number:  88 and the loss : 0.5162860751152039\n",
      "Epoch number:  89 and the loss : 0.5151454210281372\n",
      "Epoch number:  90 and the loss : 0.514445424079895\n",
      "Epoch number:  91 and the loss : 0.5135970711708069\n",
      "Epoch number:  92 and the loss : 0.5128321051597595\n",
      "Epoch number:  93 and the loss : 0.5120272040367126\n",
      "Epoch number:  94 and the loss : 0.511235237121582\n",
      "Epoch number:  95 and the loss : 0.5103038549423218\n",
      "Epoch number:  96 and the loss : 0.5096932649612427\n",
      "Epoch number:  97 and the loss : 0.5088468194007874\n",
      "Epoch number:  98 and the loss : 0.5079116821289062\n",
      "Epoch number:  99 and the loss : 0.5072685480117798\n",
      "Epoch number:  100 and the loss : 0.5063005089759827\n",
      "Epoch number:  101 and the loss : 0.5061254501342773\n",
      "Epoch number:  102 and the loss : 0.5061097145080566\n",
      "Epoch number:  103 and the loss : 0.5062465071678162\n",
      "Epoch number:  104 and the loss : 0.5029118657112122\n",
      "Epoch number:  105 and the loss : 0.5044224262237549\n",
      "Epoch number:  106 and the loss : 0.5061419010162354\n",
      "Epoch number:  107 and the loss : 0.5027725100517273\n",
      "Epoch number:  108 and the loss : 0.5017753839492798\n",
      "Epoch number:  109 and the loss : 0.5033261775970459\n",
      "Epoch number:  110 and the loss : 0.5002336502075195\n",
      "Epoch number:  111 and the loss : 0.49834126234054565\n",
      "Epoch number:  112 and the loss : 0.5006694197654724\n",
      "Epoch number:  113 and the loss : 0.4956144094467163\n",
      "Epoch number:  114 and the loss : 0.4991630017757416\n",
      "Epoch number:  115 and the loss : 0.49857378005981445\n",
      "Epoch number:  116 and the loss : 0.4951629042625427\n",
      "Epoch number:  117 and the loss : 0.4967760145664215\n",
      "Epoch number:  118 and the loss : 0.49483129382133484\n",
      "Epoch number:  119 and the loss : 0.49224337935447693\n",
      "Epoch number:  120 and the loss : 0.4958082437515259\n",
      "Epoch number:  121 and the loss : 0.49605482816696167\n",
      "Epoch number:  122 and the loss : 0.4918607473373413\n",
      "Epoch number:  123 and the loss : 0.49354681372642517\n",
      "Epoch number:  124 and the loss : 0.49024829268455505\n",
      "Epoch number:  125 and the loss : 0.4878861606121063\n",
      "Epoch number:  126 and the loss : 0.48953375220298767\n",
      "Epoch number:  127 and the loss : 0.48562949895858765\n",
      "Epoch number:  128 and the loss : 0.4859106242656708\n",
      "Epoch number:  129 and the loss : 0.4845929443836212\n",
      "Epoch number:  130 and the loss : 0.48301810026168823\n",
      "Epoch number:  131 and the loss : 0.4828636348247528\n",
      "Epoch number:  132 and the loss : 0.4814947247505188\n",
      "Epoch number:  133 and the loss : 0.481079638004303\n",
      "Epoch number:  134 and the loss : 0.48007604479789734\n",
      "Epoch number:  135 and the loss : 0.4795570969581604\n",
      "Epoch number:  136 and the loss : 0.4785955846309662\n",
      "Epoch number:  137 and the loss : 0.47819602489471436\n",
      "Epoch number:  138 and the loss : 0.47747036814689636\n",
      "Epoch number:  139 and the loss : 0.47728264331817627\n",
      "Epoch number:  140 and the loss : 0.47669121623039246\n",
      "Epoch number:  141 and the loss : 0.4756038784980774\n",
      "Epoch number:  142 and the loss : 0.47473570704460144\n",
      "Epoch number:  143 and the loss : 0.4768145978450775\n",
      "Epoch number:  144 and the loss : 0.4829734265804291\n",
      "Epoch number:  145 and the loss : 0.47699272632598877\n",
      "Epoch number:  146 and the loss : 0.477778822183609\n",
      "Epoch number:  147 and the loss : 0.4953562617301941\n",
      "Epoch number:  148 and the loss : 0.47870343923568726\n",
      "Epoch number:  149 and the loss : 0.47677603363990784\n",
      "Epoch number:  150 and the loss : 0.47578513622283936\n",
      "Epoch number:  151 and the loss : 0.4799228608608246\n",
      "Epoch number:  152 and the loss : 0.481850802898407\n",
      "Epoch number:  153 and the loss : 0.4733358323574066\n",
      "Epoch number:  154 and the loss : 0.4761788547039032\n",
      "Epoch number:  155 and the loss : 0.4889828562736511\n",
      "Epoch number:  156 and the loss : 0.4669572114944458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  157 and the loss : 0.4874245226383209\n",
      "Epoch number:  158 and the loss : 0.5034086108207703\n",
      "Epoch number:  159 and the loss : 0.4796396493911743\n",
      "Epoch number:  160 and the loss : 0.5076741576194763\n",
      "Epoch number:  161 and the loss : 0.479302316904068\n",
      "Epoch number:  162 and the loss : 0.48267412185668945\n",
      "Epoch number:  163 and the loss : 0.5006633996963501\n",
      "Epoch number:  164 and the loss : 0.47311052680015564\n",
      "Epoch number:  165 and the loss : 0.48866236209869385\n",
      "Epoch number:  166 and the loss : 0.47210463881492615\n",
      "Epoch number:  167 and the loss : 0.481431782245636\n",
      "Epoch number:  168 and the loss : 0.46870899200439453\n",
      "Epoch number:  169 and the loss : 0.47235924005508423\n",
      "Epoch number:  170 and the loss : 0.47330546379089355\n",
      "Epoch number:  171 and the loss : 0.4650035500526428\n",
      "Epoch number:  172 and the loss : 0.47160622477531433\n",
      "Epoch number:  173 and the loss : 0.46218252182006836\n",
      "Epoch number:  174 and the loss : 0.47070083022117615\n",
      "Epoch number:  175 and the loss : 0.46188175678253174\n",
      "Epoch number:  176 and the loss : 0.4629262089729309\n",
      "Epoch number:  177 and the loss : 0.4626213312149048\n",
      "Epoch number:  178 and the loss : 0.4591902792453766\n",
      "Epoch number:  179 and the loss : 0.4628400206565857\n",
      "Epoch number:  180 and the loss : 0.4574800133705139\n",
      "Epoch number:  181 and the loss : 0.4582119882106781\n",
      "Epoch number:  182 and the loss : 0.45852938294410706\n",
      "Epoch number:  183 and the loss : 0.45752230286598206\n",
      "Epoch number:  184 and the loss : 0.45629802346229553\n",
      "Epoch number:  185 and the loss : 0.45471078157424927\n",
      "Epoch number:  186 and the loss : 0.4569934904575348\n",
      "Epoch number:  187 and the loss : 0.45312729477882385\n",
      "Epoch number:  188 and the loss : 0.4535970687866211\n",
      "Epoch number:  189 and the loss : 0.4530270993709564\n",
      "Epoch number:  190 and the loss : 0.45109885931015015\n",
      "Epoch number:  191 and the loss : 0.45270317792892456\n",
      "Epoch number:  192 and the loss : 0.4508834481239319\n",
      "Epoch number:  193 and the loss : 0.4501897990703583\n",
      "Epoch number:  194 and the loss : 0.45014336705207825\n",
      "Epoch number:  195 and the loss : 0.44840750098228455\n",
      "Epoch number:  196 and the loss : 0.44797638058662415\n",
      "Epoch number:  197 and the loss : 0.4481358230113983\n",
      "Epoch number:  198 and the loss : 0.4469020962715149\n",
      "Epoch number:  199 and the loss : 0.44622603058815\n",
      "Epoch number:  200 and the loss : 0.4470039904117584\n",
      "Epoch number:  201 and the loss : 0.44579964876174927\n",
      "Epoch number:  202 and the loss : 0.4441184997558594\n",
      "Epoch number:  203 and the loss : 0.4442129135131836\n",
      "Epoch number:  204 and the loss : 0.4442789852619171\n",
      "Epoch number:  205 and the loss : 0.44239217042922974\n",
      "Epoch number:  206 and the loss : 0.4423321783542633\n",
      "Epoch number:  207 and the loss : 0.44232994318008423\n",
      "Epoch number:  208 and the loss : 0.44132059812545776\n",
      "Epoch number:  209 and the loss : 0.4402579069137573\n",
      "Epoch number:  210 and the loss : 0.4398637115955353\n",
      "Epoch number:  211 and the loss : 0.44026392698287964\n",
      "Epoch number:  212 and the loss : 0.44014090299606323\n",
      "Epoch number:  213 and the loss : 0.43902772665023804\n",
      "Epoch number:  214 and the loss : 0.4372478127479553\n",
      "Epoch number:  215 and the loss : 0.436678409576416\n",
      "Epoch number:  216 and the loss : 0.436314195394516\n",
      "Epoch number:  217 and the loss : 0.43823152780532837\n",
      "Epoch number:  218 and the loss : 0.4375428557395935\n",
      "Epoch number:  219 and the loss : 0.4354800879955292\n",
      "Epoch number:  220 and the loss : 0.43573230504989624\n",
      "Epoch number:  221 and the loss : 0.4361862540245056\n",
      "Epoch number:  222 and the loss : 0.4364783465862274\n",
      "Epoch number:  223 and the loss : 0.43295562267303467\n",
      "Epoch number:  224 and the loss : 0.4334653317928314\n",
      "Epoch number:  225 and the loss : 0.43148863315582275\n",
      "Epoch number:  226 and the loss : 0.43174734711647034\n",
      "Epoch number:  227 and the loss : 0.4298151731491089\n",
      "Epoch number:  228 and the loss : 0.4298854172229767\n",
      "Epoch number:  229 and the loss : 0.4290007948875427\n",
      "Epoch number:  230 and the loss : 0.4295755624771118\n",
      "Epoch number:  231 and the loss : 0.4289281368255615\n",
      "Epoch number:  232 and the loss : 0.43015649914741516\n",
      "Epoch number:  233 and the loss : 0.43254125118255615\n",
      "Epoch number:  234 and the loss : 0.43669283390045166\n",
      "Epoch number:  235 and the loss : 0.4389443099498749\n",
      "Epoch number:  236 and the loss : 0.43919655680656433\n",
      "Epoch number:  237 and the loss : 0.4283026158809662\n",
      "Epoch number:  238 and the loss : 0.4255107045173645\n",
      "Epoch number:  239 and the loss : 0.42448246479034424\n",
      "Epoch number:  240 and the loss : 0.4271993637084961\n",
      "Epoch number:  241 and the loss : 0.43041887879371643\n",
      "Epoch number:  242 and the loss : 0.43227338790893555\n",
      "Epoch number:  243 and the loss : 0.4309634268283844\n",
      "Epoch number:  244 and the loss : 0.4322074353694916\n",
      "Epoch number:  245 and the loss : 0.4364236295223236\n",
      "Epoch number:  246 and the loss : 0.4364814758300781\n",
      "Epoch number:  247 and the loss : 0.43028557300567627\n",
      "Epoch number:  248 and the loss : 0.42811280488967896\n",
      "Epoch number:  249 and the loss : 0.42016011476516724\n",
      "Epoch number:  250 and the loss : 0.42587095499038696\n",
      "Epoch number:  251 and the loss : 0.42554813623428345\n",
      "Epoch number:  252 and the loss : 0.42683520913124084\n",
      "Epoch number:  253 and the loss : 0.4309098422527313\n",
      "Epoch number:  254 and the loss : 0.44966816902160645\n",
      "Epoch number:  255 and the loss : 0.4518221914768219\n",
      "Epoch number:  256 and the loss : 0.4295102059841156\n",
      "Epoch number:  257 and the loss : 0.4318525493144989\n",
      "Epoch number:  258 and the loss : 0.4263598918914795\n",
      "Epoch number:  259 and the loss : 0.43703940510749817\n",
      "Epoch number:  260 and the loss : 0.43324682116508484\n",
      "Epoch number:  261 and the loss : 0.42625850439071655\n",
      "Epoch number:  262 and the loss : 0.4306211769580841\n",
      "Epoch number:  263 and the loss : 0.4208236336708069\n",
      "Epoch number:  264 and the loss : 0.42775091528892517\n",
      "Epoch number:  265 and the loss : 0.42046815156936646\n",
      "Epoch number:  266 and the loss : 0.42016586661338806\n",
      "Epoch number:  267 and the loss : 0.41919541358947754\n",
      "Epoch number:  268 and the loss : 0.4215030372142792\n",
      "Epoch number:  269 and the loss : 0.41782212257385254\n",
      "Epoch number:  270 and the loss : 0.41450443863868713\n",
      "Epoch number:  271 and the loss : 0.4194726049900055\n",
      "Epoch number:  272 and the loss : 0.4130987226963043\n",
      "Epoch number:  273 and the loss : 0.4140879809856415\n",
      "Epoch number:  274 and the loss : 0.41345134377479553\n",
      "Epoch number:  275 and the loss : 0.41443467140197754\n",
      "Epoch number:  276 and the loss : 0.41508543491363525\n",
      "Epoch number:  277 and the loss : 0.4169677197933197\n",
      "Epoch number:  278 and the loss : 0.42382535338401794\n",
      "Epoch number:  279 and the loss : 0.4233512878417969\n",
      "Epoch number:  280 and the loss : 0.42726612091064453\n",
      "Epoch number:  281 and the loss : 0.4297040104866028\n",
      "Epoch number:  282 and the loss : 0.433339923620224\n",
      "Epoch number:  283 and the loss : 0.42313164472579956\n",
      "Epoch number:  284 and the loss : 0.4079917371273041\n",
      "Epoch number:  285 and the loss : 0.4112641513347626\n",
      "Epoch number:  286 and the loss : 0.4192138612270355\n",
      "Epoch number:  287 and the loss : 0.4346708059310913\n",
      "Epoch number:  288 and the loss : 0.43138739466667175\n",
      "Epoch number:  289 and the loss : 0.4065752327442169\n",
      "Epoch number:  290 and the loss : 0.4143790602684021\n",
      "Epoch number:  291 and the loss : 0.42822498083114624\n",
      "Epoch number:  292 and the loss : 0.4634365737438202\n",
      "Epoch number:  293 and the loss : 0.44416263699531555\n",
      "Epoch number:  294 and the loss : 0.4076537787914276\n",
      "Epoch number:  295 and the loss : 0.4519447684288025\n",
      "Epoch number:  296 and the loss : 0.48705971240997314\n",
      "Epoch number:  297 and the loss : 0.4921931028366089\n",
      "Epoch number:  298 and the loss : 0.46371495723724365\n",
      "Epoch number:  299 and the loss : 0.4693616032600403\n",
      "Epoch number:  300 and the loss : 0.43886256217956543\n",
      "Epoch number:  301 and the loss : 0.4414214491844177\n",
      "Epoch number:  302 and the loss : 0.42939430475234985\n",
      "Epoch number:  303 and the loss : 0.4317448139190674\n",
      "Epoch number:  304 and the loss : 0.43462279438972473\n",
      "Epoch number:  305 and the loss : 0.42972180247306824\n",
      "Epoch number:  306 and the loss : 0.42287179827690125\n",
      "Epoch number:  307 and the loss : 0.425687700510025\n",
      "Epoch number:  308 and the loss : 0.41674643754959106\n",
      "Epoch number:  309 and the loss : 0.41224172711372375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  310 and the loss : 0.41752541065216064\n",
      "Epoch number:  311 and the loss : 0.41391947865486145\n",
      "Epoch number:  312 and the loss : 0.40688556432724\n",
      "Epoch number:  313 and the loss : 0.4119664430618286\n",
      "Epoch number:  314 and the loss : 0.4064942002296448\n",
      "Epoch number:  315 and the loss : 0.4037376642227173\n",
      "Epoch number:  316 and the loss : 0.40626707673072815\n",
      "Epoch number:  317 and the loss : 0.4038506746292114\n",
      "Epoch number:  318 and the loss : 0.40256762504577637\n",
      "Epoch number:  319 and the loss : 0.4023597538471222\n",
      "Epoch number:  320 and the loss : 0.4004814624786377\n",
      "Epoch number:  321 and the loss : 0.40018486976623535\n",
      "Epoch number:  322 and the loss : 0.3994886875152588\n",
      "Epoch number:  323 and the loss : 0.397845596075058\n",
      "Epoch number:  324 and the loss : 0.3978191614151001\n",
      "Epoch number:  325 and the loss : 0.39706099033355713\n",
      "Epoch number:  326 and the loss : 0.3965420722961426\n",
      "Epoch number:  327 and the loss : 0.39716973900794983\n",
      "Epoch number:  328 and the loss : 0.39812421798706055\n",
      "Epoch number:  329 and the loss : 0.3968462646007538\n",
      "Epoch number:  330 and the loss : 0.39540374279022217\n",
      "Epoch number:  331 and the loss : 0.3938528001308441\n",
      "Epoch number:  332 and the loss : 0.3934938311576843\n",
      "Epoch number:  333 and the loss : 0.39454853534698486\n",
      "Epoch number:  334 and the loss : 0.3922061622142792\n",
      "Epoch number:  335 and the loss : 0.39193636178970337\n",
      "Epoch number:  336 and the loss : 0.39286667108535767\n",
      "Epoch number:  337 and the loss : 0.39134395122528076\n",
      "Epoch number:  338 and the loss : 0.3908359408378601\n",
      "Epoch number:  339 and the loss : 0.38976407051086426\n",
      "Epoch number:  340 and the loss : 0.38993707299232483\n",
      "Epoch number:  341 and the loss : 0.38938695192337036\n",
      "Epoch number:  342 and the loss : 0.38823193311691284\n",
      "Epoch number:  343 and the loss : 0.38828375935554504\n",
      "Epoch number:  344 and the loss : 0.38800835609436035\n",
      "Epoch number:  345 and the loss : 0.38840925693511963\n",
      "Epoch number:  346 and the loss : 0.3879225552082062\n",
      "Epoch number:  347 and the loss : 0.3874516487121582\n",
      "Epoch number:  348 and the loss : 0.3871683180332184\n",
      "Epoch number:  349 and the loss : 0.38666093349456787\n",
      "Epoch number:  350 and the loss : 0.3861958980560303\n",
      "Epoch number:  351 and the loss : 0.3859727680683136\n",
      "Epoch number:  352 and the loss : 0.38701340556144714\n",
      "Epoch number:  353 and the loss : 0.3884786069393158\n",
      "Epoch number:  354 and the loss : 0.3927142322063446\n",
      "Epoch number:  355 and the loss : 0.3983559012413025\n",
      "Epoch number:  356 and the loss : 0.40870705246925354\n",
      "Epoch number:  357 and the loss : 0.4254862666130066\n",
      "Epoch number:  358 and the loss : 0.4398636817932129\n",
      "Epoch number:  359 and the loss : 0.4314480721950531\n",
      "Epoch number:  360 and the loss : 0.40622395277023315\n",
      "Epoch number:  361 and the loss : 0.385122150182724\n",
      "Epoch number:  362 and the loss : 0.3889240026473999\n",
      "Epoch number:  363 and the loss : 0.4091070294380188\n",
      "Epoch number:  364 and the loss : 0.4228549897670746\n",
      "Epoch number:  365 and the loss : 0.412028044462204\n",
      "Epoch number:  366 and the loss : 0.38621386885643005\n",
      "Epoch number:  367 and the loss : 0.39024829864501953\n",
      "Epoch number:  368 and the loss : 0.41827869415283203\n",
      "Epoch number:  369 and the loss : 0.43251731991767883\n",
      "Epoch number:  370 and the loss : 0.41018059849739075\n",
      "Epoch number:  371 and the loss : 0.3849424123764038\n",
      "Epoch number:  372 and the loss : 0.40322554111480713\n",
      "Epoch number:  373 and the loss : 0.42055007815361023\n",
      "Epoch number:  374 and the loss : 0.40186065435409546\n",
      "Epoch number:  375 and the loss : 0.38129425048828125\n",
      "Epoch number:  376 and the loss : 0.39294949173927307\n",
      "Epoch number:  377 and the loss : 0.40470975637435913\n",
      "Epoch number:  378 and the loss : 0.38791295886039734\n",
      "Epoch number:  379 and the loss : 0.37878793478012085\n",
      "Epoch number:  380 and the loss : 0.39197099208831787\n",
      "Epoch number:  381 and the loss : 0.395134299993515\n",
      "Epoch number:  382 and the loss : 0.38509270548820496\n",
      "Epoch number:  383 and the loss : 0.3785298764705658\n",
      "Epoch number:  384 and the loss : 0.3828105330467224\n",
      "Epoch number:  385 and the loss : 0.38622352480888367\n",
      "Epoch number:  386 and the loss : 0.3805497884750366\n",
      "Epoch number:  387 and the loss : 0.376648485660553\n",
      "Epoch number:  388 and the loss : 0.37671148777008057\n",
      "Epoch number:  389 and the loss : 0.37996891140937805\n",
      "Epoch number:  390 and the loss : 0.37753647565841675\n",
      "Epoch number:  391 and the loss : 0.3749665319919586\n",
      "Epoch number:  392 and the loss : 0.3756450414657593\n",
      "Epoch number:  393 and the loss : 0.3772977292537689\n",
      "Epoch number:  394 and the loss : 0.37888482213020325\n",
      "Epoch number:  395 and the loss : 0.3763824999332428\n",
      "Epoch number:  396 and the loss : 0.3735235333442688\n",
      "Epoch number:  397 and the loss : 0.37365955114364624\n",
      "Epoch number:  398 and the loss : 0.374520480632782\n",
      "Epoch number:  399 and the loss : 0.3743624687194824\n",
      "Epoch number:  400 and the loss : 0.3732188940048218\n",
      "Epoch number:  401 and the loss : 0.37265101075172424\n",
      "Epoch number:  402 and the loss : 0.37126365303993225\n",
      "Epoch number:  403 and the loss : 0.37165990471839905\n",
      "Epoch number:  404 and the loss : 0.3717907667160034\n",
      "Epoch number:  405 and the loss : 0.3723888397216797\n",
      "Epoch number:  406 and the loss : 0.37229397892951965\n",
      "Epoch number:  407 and the loss : 0.3720451593399048\n",
      "Epoch number:  408 and the loss : 0.37176939845085144\n",
      "Epoch number:  409 and the loss : 0.3715514838695526\n",
      "Epoch number:  410 and the loss : 0.3715679347515106\n",
      "Epoch number:  411 and the loss : 0.3715188801288605\n",
      "Epoch number:  412 and the loss : 0.3717462122440338\n",
      "Epoch number:  413 and the loss : 0.3718811273574829\n",
      "Epoch number:  414 and the loss : 0.37239712476730347\n",
      "Epoch number:  415 and the loss : 0.37268972396850586\n",
      "Epoch number:  416 and the loss : 0.3736031949520111\n",
      "Epoch number:  417 and the loss : 0.374461829662323\n",
      "Epoch number:  418 and the loss : 0.37547171115875244\n",
      "Epoch number:  419 and the loss : 0.37660518288612366\n",
      "Epoch number:  420 and the loss : 0.3783535659313202\n",
      "Epoch number:  421 and the loss : 0.3818484842777252\n",
      "Epoch number:  422 and the loss : 0.38802456855773926\n",
      "Epoch number:  423 and the loss : 0.3920958340167999\n",
      "Epoch number:  424 and the loss : 0.39249444007873535\n",
      "Epoch number:  425 and the loss : 0.39026400446891785\n",
      "Epoch number:  426 and the loss : 0.38287773728370667\n",
      "Epoch number:  427 and the loss : 0.3731054663658142\n",
      "Epoch number:  428 and the loss : 0.367220938205719\n",
      "Epoch number:  429 and the loss : 0.36612552404403687\n",
      "Epoch number:  430 and the loss : 0.36870089173316956\n",
      "Epoch number:  431 and the loss : 0.37339359521865845\n",
      "Epoch number:  432 and the loss : 0.3786574900150299\n",
      "Epoch number:  433 and the loss : 0.385753870010376\n",
      "Epoch number:  434 and the loss : 0.3888413608074188\n",
      "Epoch number:  435 and the loss : 0.38744521141052246\n",
      "Epoch number:  436 and the loss : 0.38018858432769775\n",
      "Epoch number:  437 and the loss : 0.3730818033218384\n",
      "Epoch number:  438 and the loss : 0.3686583936214447\n",
      "Epoch number:  439 and the loss : 0.36441507935523987\n",
      "Epoch number:  440 and the loss : 0.3646392822265625\n",
      "Epoch number:  441 and the loss : 0.3667682409286499\n",
      "Epoch number:  442 and the loss : 0.3691190183162689\n",
      "Epoch number:  443 and the loss : 0.37232235074043274\n",
      "Epoch number:  444 and the loss : 0.37625065445899963\n",
      "Epoch number:  445 and the loss : 0.37986040115356445\n",
      "Epoch number:  446 and the loss : 0.37972375750541687\n",
      "Epoch number:  447 and the loss : 0.37708988785743713\n",
      "Epoch number:  448 and the loss : 0.3745494782924652\n",
      "Epoch number:  449 and the loss : 0.37142929434776306\n",
      "Epoch number:  450 and the loss : 0.3688293993473053\n",
      "Epoch number:  451 and the loss : 0.36699730157852173\n",
      "Epoch number:  452 and the loss : 0.36450180411338806\n",
      "Epoch number:  453 and the loss : 0.3626367449760437\n",
      "Epoch number:  454 and the loss : 0.36113470792770386\n",
      "Epoch number:  455 and the loss : 0.36092376708984375\n",
      "Epoch number:  456 and the loss : 0.3599075675010681\n",
      "Epoch number:  457 and the loss : 0.35995641350746155\n",
      "Epoch number:  458 and the loss : 0.3597159683704376\n",
      "Epoch number:  459 and the loss : 0.35876116156578064\n",
      "Epoch number:  460 and the loss : 0.3591386675834656\n",
      "Epoch number:  461 and the loss : 0.35898149013519287\n",
      "Epoch number:  462 and the loss : 0.35931292176246643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  463 and the loss : 0.3609304130077362\n",
      "Epoch number:  464 and the loss : 0.3652142584323883\n",
      "Epoch number:  465 and the loss : 0.37734338641166687\n",
      "Epoch number:  466 and the loss : 0.40137505531311035\n",
      "Epoch number:  467 and the loss : 0.4392721951007843\n",
      "Epoch number:  468 and the loss : 0.46677365899086\n",
      "Epoch number:  469 and the loss : 0.43502187728881836\n",
      "Epoch number:  470 and the loss : 0.3703891336917877\n",
      "Epoch number:  471 and the loss : 0.36999544501304626\n",
      "Epoch number:  472 and the loss : 0.41694730520248413\n",
      "Epoch number:  473 and the loss : 0.45687299966812134\n",
      "Epoch number:  474 and the loss : 0.4189334809780121\n",
      "Epoch number:  475 and the loss : 0.3641641139984131\n",
      "Epoch number:  476 and the loss : 0.41298145055770874\n",
      "Epoch number:  477 and the loss : 0.4528067409992218\n",
      "Epoch number:  478 and the loss : 0.41265150904655457\n",
      "Epoch number:  479 and the loss : 0.37390968203544617\n",
      "Epoch number:  480 and the loss : 0.39839163422584534\n",
      "Epoch number:  481 and the loss : 0.404977411031723\n",
      "Epoch number:  482 and the loss : 0.37246742844581604\n",
      "Epoch number:  483 and the loss : 0.3732924163341522\n",
      "Epoch number:  484 and the loss : 0.39838138222694397\n",
      "Epoch number:  485 and the loss : 0.38140732049942017\n",
      "Epoch number:  486 and the loss : 0.3619534969329834\n",
      "Epoch number:  487 and the loss : 0.3735480308532715\n",
      "Epoch number:  488 and the loss : 0.3723282814025879\n",
      "Epoch number:  489 and the loss : 0.3627729117870331\n",
      "Epoch number:  490 and the loss : 0.3601290285587311\n",
      "Epoch number:  491 and the loss : 0.36840155720710754\n",
      "Epoch number:  492 and the loss : 0.3621298372745514\n",
      "Epoch number:  493 and the loss : 0.35524889826774597\n",
      "Epoch number:  494 and the loss : 0.3670954704284668\n",
      "Epoch number:  495 and the loss : 0.3564702570438385\n",
      "Epoch number:  496 and the loss : 0.3638147711753845\n",
      "Epoch number:  497 and the loss : 0.36222830414772034\n",
      "Epoch number:  498 and the loss : 0.35778433084487915\n",
      "Epoch number:  499 and the loss : 0.35625386238098145\n",
      "Epoch number:  500 and the loss : 0.36730077862739563\n"
     ]
    }
   ],
   "source": [
    "epochs=500\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred=model.forward(X_train)\n",
    "    loss=loss_function(y_pred,y_train)\n",
    "    final_losses.append(loss)\n",
    "    if 1%10==1:\n",
    "        print(\"Epoch number:  {} and the loss : {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the loss function\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZTddX3/8ef7brNvmZnsG1nYBYIhBLE2BW0jtVgrLmCVWjm0VCttrT+1PQePnmq1tkrRHi0udbduUFMKsgkCRwSGkARCEkhCQibrZJLZtzt33r8/7ncmd+7cTCbLN3cm39fjnHvu/X6/n7n38w3Dfc1n+X6+5u6IiEh0xYpdARERKS4FgYhIxCkIREQiTkEgIhJxCgIRkYhLFLsCx6uhocEXLlxY7GqIiEwpzz777EF3byx0bMoFwcKFC2lqaip2NUREphQz23m0Y+oaEhGJOAWBiEjEhRYEZlZqZk+b2Xoz22hmnypQ5s/MrMXM1gWPm8Kqj4iIFBbmGEE/cJW7d5lZEnjCzO5z99/mlfuxu38oxHqIiMg4QgsCzy5i1BVsJoOHFjYSEZlkQh0jMLO4ma0DDgAPuvtTBYq93cw2mNnPzGxemPUREZGxQg0Cd8+4+yXAXGCFmV2YV+R/gYXufhHwEPCdQu9jZjebWZOZNbW0tIRZZRGRyDkts4bcvQ14FFidt7/V3fuDza8Drz3Kz9/p7svdfXljY8HrIY5py75O/u2BLRzs6j92YRGRCAlz1lCjmdUGr8uANwKb88rMytm8FtgUVn22Hujiy7/ayqHugbA+QkRkSgpz1tAs4DtmFicbOD9x93vM7NNAk7uvAT5sZtcCg8Ah4M/CqoxZ9nlIN+IRERklzFlDG4BlBfbflvP6E8AnwqpDrpgNf+bp+DQRkakjQlcWZ5NALQIRkdEiEwSmFoGISEGRCYLYcBKIiMgokQmC4RhQ15CIyGiRCYJYcKbKARGR0SITBKbBYhGRgqITBMODxcWthojIpBOhIMgmgatFICIySnSCIHhWDoiIjBaZIBiePqocEBEZLTJBMLLW0JCiQEQkV+SCQDEgIjJadIJA00dFRAqKTBDERkaLi1oNEZFJJzJBMDx9VEMEIiKjRSYIRu5HoCaBiMgokQmCI3coK249REQmm8gEwfAlZbqyWERktMgEQUzTR0VECopMEGitIRGRwiITBLp5vYhIYZEJgiMXlBW5IiIik0x0gmCkRaAkEBHJFVoQmFmpmT1tZuvNbKOZfapAmRIz+7GZbTWzp8xsYXj1yT6rRSAiMlqYLYJ+4Cp3vxi4BFhtZivzynwAOOzuS4AvAZ8PqzKG1pgQESkktCDwrK5gMxk88r+F3wp8J3j9M+BqG57ec4rp5vUiIoWFOkZgZnEzWwccAB5096fyiswBdgG4+yDQDtQXeJ+bzazJzJpaWlpOrC4aLBYRKSjUIHD3jLtfAswFVpjZhXlFCv31P+ar2t3vdPfl7r68sbHxhOqitYZERAo7LbOG3L0NeBRYnXeoGZgHYGYJoAY4FEYdNFgsIlJYmLOGGs2sNnhdBrwR2JxXbA1wY/D6OuBXHtL8Tl1ZLCJSWCLE954FfMfM4mQD5yfufo+ZfRpocvc1wDeB75nZVrItgXeHVZmROUPKARGRUUILAnffACwrsP+2nNd9wDvCqkOu2HCLQGMEIiKjRO7K4qGh4tZDRGSyiU4QDN+PoMj1EBGZbKITBFprSESkoAgGQXHrISIy2UQmCDRYLCJSWGSCQBeUiYgUFpkgGGkRKAhEREaJTBAMX1A2pCQQERklMkHAyKJzIiKSKzJBENO0IRGRgiITBEe6hopaDRGRSScyQRDT6qMiIgVFJgg0fVREpLAIBYHWGhIRKSRCQZB9VteQiMhokQkCXVAmIlJYZIJAF5SJiBQWnSDQBWUiIgVFJgjUNSQiUlhkgmCYuoZEREaLTBCMLDEhIiKjRCYIjty8Xi0CEZFckQmCmC4oExEpKLQgMLN5ZvaImW0ys41mdmuBMqvMrN3M1gWP20KrT/CsMQIRkdESIb73IPARd19rZlXAs2b2oLu/mFfucXd/S4j1AHTzehGRowmtReDue919bfC6E9gEzAnr845Faw2JiBR2WsYIzGwhsAx4qsDhK8xsvZndZ2YXHOXnbzazJjNramlpOYl6aK0hEZF8oQeBmVUCPwf+xt078g6vBRa4+8XAl4H/KfQe7n6nuy939+WNjY0nXJeYmbqGRETyhBoEZpYkGwI/cPe78o+7e4e7dwWv7wWSZtYQWn3QYLGISL4wZw0Z8E1gk7t/8ShlZgblMLMVQX1aw6pTzExjBCIiecKcNXQl8F7geTNbF+z7B2A+gLt/DbgOuMXMBoFe4N0eZie+qUUgIpIvtCBw9yc4Mn3/aGW+AnwlrDrkixmaNiQikicyVxYDGKYWgYhInmgFgemCMhGRfJEKAg0Wi4iMFakg0PRREZGxohUE6hoSERkjYkFgWmJCRCRPpIIgZpo9KiKSL1JBYKbpoyIi+aIVBGiMQEQkX7SCQNNHRUTGiFgQ6H4EIiL5IhUEMU0fFREZI1JBoLWGRETGilQQqEUgIjJWpIIgO3202LUQEZlcIhUEAK55QyIio0QqCGIxdQ2JiOSLVBAYWmtIRCRfpIJAaw2JiIwVqSDQYLGIyFgRCwJdWSwiki9aQYAGi0VE8kUqCLL3LFYSiIjkCi0IzGyemT1iZpvMbKOZ3VqgjJnZHWa21cw2mNmlYdUn+3kwNBTmJ4iITD0TCgIzW2xmJcHrVWb2YTOrPcaPDQIfcffzgJXAB83s/LwybwaWBo+bga8eV+2Pk6EWgYhIvom2CH4OZMxsCfBN4Czgh+P9gLvvdfe1wetOYBMwJ6/YW4HvetZvgVozm3U8J3A8dPN6EZGxJhoEQ+4+CLwNuN3d/xaY8Be2mS0ElgFP5R2aA+zK2W5mbFhgZjebWZOZNbW0tEz0YwvVQ9NHRUTyTDQI0mZ2PXAjcE+wLzmRHzSzSrItir9x9478wwV+ZMxXtbvf6e7L3X15Y2PjBKs8VswKvr2ISKRNNAjeD1wBfMbdXzGzs4DvH+uHzCxJNgR+4O53FSjSDMzL2Z4L7JlgnY6bGWoRiIjkmVAQuPuL7v5hd/+RmdUBVe7+ufF+xsyM7HjCJnf/4lGKrQHeF8weWgm0u/ve4zmB4xEzrTUkIpIvMZFCZvYocG1Qfh3QYma/dve/G+fHrgTeCzxvZuuCff8AzAdw968B9wLXAFuBHrItj9AYahGIiOSbUBAANe7eYWY3Af/l7p80sw3j/YC7P0HhMYDcMg58cIJ1OHlmGiEQEckz0TGCRDCt850cGSyecmJaa0hEZIyJBsGngfuBbe7+jJktAl4Or1rh0FpDIiJjTahryN1/Cvw0Z3s78PawKhUWrTUkIjLWRJeYmGtmd5vZATPbb2Y/N7O5YVfuVNNaQyIiY020a+i/yE71nE32yt//DfZNKaYWgYjIGBMNgkZ3/y93Hwwe3wZO/BLfItH0URGRsSYaBAfN7E/NLB48/hRoDbNiYYiZblosIpJvokHw52Snju4D9gLXEfLFX2HILjGhJBARyTXRJSZedfdr3b3R3ae7+x8DfxJy3U45NQhERMY6mTuUjbe8xKQUM1OLQEQkz8kEwbjLR0xGiZgxmFEQiIjkOpkgmHLfqKlEjP7BTLGrISIyqYx7ZbGZdVL4C9+AslBqFKKSRJyBQV1RJiKSa9wgcPeq01WR0yGViCkIRETynEzX0JSTSsQYyCgIRERyRSsI4jH61SIQERklUkFQoq4hEZExIhUEw11DujmNiMgR0QqCeAx3GNTKcyIiI6IVBIns6ap7SETkiEgGgQaMRUSOiGQQqEUgInJEpIKgJBEHFAQiIrlCCwIz+1Zwj+MXjnJ8lZm1m9m64HFbWHUZNtIiyGi9IRGRYeMuMXGSvg18BfjuOGUed/e3hFiHUVJxjRGIiOQLrUXg7o8Bh8J6/xNRojECEZExij1GcIWZrTez+8zsgqMVMrObzazJzJpaWlpO+MM0WCwiMlYxg2AtsMDdLwa+DPzP0Qq6+53uvtzdlzc2Np7wBx4ZI1AQiIgMK1oQuHuHu3cFr+8FkmbWEOZnDo8RqEUgInJE0YLAzGaamQWvVwR1aQ3zM9U1JCIyVmizhszsR8AqoMHMmoFPAkkAd/8acB1wi5kNAr3Auz3k1eB0ZbGIyFihBYG7X3+M418hO730tFHXkIjIWMWeNXRaDbcI0kMKAhGRYZEKgmTQIhjMaBlqEZFhkQqCRNwASGv6qIjIiEgFQTIWdA2pRSAiMiJSQTDcIhhUi0BEZES0giAWdA3pVpUiIiMiFQRmRjJuGiMQEckRqSAASMRi6hoSEckRuSDItgjUNSQiMiyCQRBT15CISI7IBUEibrqgTEQkR+SCIBmPaYkJEZEckQwCtQhERI6IXBAkYpo+KiKSK3JBkB0sVotARGRYBIPAeGjTfr74wJZiV0VEZFKIXBAkgqWo7/jV1iLXRERkcoheEATrDYmISFbkgsCUAyIio0QuCDR1VERktMgFwYCmjoqIjBK9IBhUEIiI5IpcEOhiMhGR0UILAjP7lpkdMLMXjnLczOwOM9tqZhvM7NKw6pJLF5OJiIwWZovg28DqcY6/GVgaPG4GvhpiXUaoRSAiMlpoQeDujwGHxinyVuC7nvVboNbMZoVVn2EaIxARGa2YYwRzgF05283BvjHM7GYzazKzppaWlpP60NxZQ+7qJhIRKWYQFLq0q+A3s7vf6e7L3X15Y2PjSX1obougX60DEZGiBkEzMC9ney6wJ+wPzR0j6E8rCEREihkEa4D3BbOHVgLt7r437A/9q1VLRl73DWbC/jgRkUkvEdYbm9mPgFVAg5k1A58EkgDu/jXgXuAaYCvQA7w/rLrk+vs/OIezGir4yE/X0zugIBARCS0I3P36Yxx34INhff54SpNxQC0CERGI4JXFAGWp7GmrRSAiEtEgqEhlG0Ld/QoCEZFIBkFVaRKAzr50kWsiIlJ8EQ2CbIugs2+wyDURESm+SAZBddAi+N8Ne+hQq0BEIi6SQVAZtAgef/kgH/nJejbt7dByEyISWZEMgnjODewffHE/b/73x/nFutAvahYRmZQiGQSFrNvVVuwqiIgUhYIg0JfWVFIRiSYFQaBbF5eJSEQpCAJ723qLXQURkaKIbBB8bPW5o7Z3KwhEJKIiGwS3rFrMe1cuGNne39Gn+xmLSCRFNggAugeyVxaXJeMMOfznr7eRGdL1BCISLZEOgkvn1wFw2x+dD8C/PvASX7h/C7c/9BLP7DhUzKqJiJw2NtWuqF2+fLk3NTWdkvdyd3oGMrR09rPqXx8FIJWIjdzXePtnryEWK3RrZRGRqcXMnnX35YWOhXZjmqnAzKgoSZCIH/myz725/dM7DjGtIsXZM6qKUT0RkdMi0i2CXPva+2jaeYgP/fC5Mce+fP0y6itTLJtXR1kqfso/W0QkbOO1CCI9RpBrZk0pr1/SwBWL6rnnr19PXXly5Nhf/+g5bvj6U/zdT9ZpcToROeOoRXAUP3zqVTbuaedty+bwhfu3sOtQD3va+5hTW0b/YIY3nT+Dj7/5PGrKksd+MxGRIhuvRaAgmKCBwSH+8I7H2d3Wy+sW1/PIlhbiMWNaeYrykjiLGiq5ckk9C+rLmVNbzrxpZZSnIj0EIyKTiAaLT4FUIsZ9t/4OAIl4jBd2t7Nm/R7aegbo7s+wblcbD23af6R8PMYbzm5g/rQK6itTzJtWzry6MuZPK2daRQozzUYSkclBQXAcEvEjQyoXzqnhwjk1I9vuTktnP81tvew+3MvaVw/z6JYWfrv9EF39o2+JWZaMM6eujDm1ZcyuLWVmdRmzakqZmfOoKkkoLETktAi1a8jMVgP/DsSBb7j75/KO/xnwBWB3sOsr7v6N8d6zWF1DJ6NnYJDmw7282trDq4d6aD7cy+62Hva297H7cC+t3QNjfqYsGSceMxqrSpheVcKM6lKmV5XQmPeoryihrjw5KqRERPIVpWvIzOLAfwBvApqBZ8xsjbu/mFf0x+7+obDqMRmUpxKcPaPqqNcj9A9mONDRz76OPva297G/vY/9HX0MDjkHu/o50NHPul1ttHT201vgvglmUFeeor4ixbSKFHXlKeoqUtSVJ6kuS5KIGTNrSmnvTXPerGoMOGdmFYlYjGTcTkvLo7t/kFcOdvP0K4d43xULyLhTktBUXJHJIMyuoRXAVnffDmBm/w28FcgPgsgrScSzYwjTysct5+50B1dCDz8OdfdzsGuA1u5+WrsGaO0eYPvBLg7tTNPWM8DgMdZOiseMObVl1FWkaKhIcbB7gEUNFSxqqGDetHLOm1XN9KoSasuTxwyM5sM9/PtDL/PR1ecwvaoUyN7w5+X9XXz0Z+vZvK8TgG//ZgeHewb49vsv49L5deoCOwGZIefna5tZMK2cyxfVF7s6MsWFGQRzgF05283A5QXKvd3M3gC8BPytu+/KL2BmNwM3A8yfPz+Eqk4NZkZlSYLKkgRnNVQcs/zwEhrdA4PsbeujoiTBjoPdDGSG2LS3gz1tfVSXJXi1tYfOvkE27+tkRnUJj2w5wN3PpUe9VyoRo7GyhJqyJNVlCapLs62N4W6p/3luNy2d/QwOOZv2dTC7poydrT3sae+ls2/0GMmrh3oAePtXnwTg7950NhfOqWb+tHLqylOUpxK6cO8YftK0i0/c9Tz1FSl++w9Xk1TXoJyE0MYIzOwdwB+4+03B9nuBFe7+1zll6oEud+83s78E3unuV433vlNxjGAqaunsZ3dbL9sOdNHem2Z/Zx8tHf109KXp6B0MntO0dg/QPzhEfUWK0mScaRUpDvcM0JceIh6D/R39I+954xULaOnqpzQZ5661u4/62XPryrh4bi3lqTgza0qZVVNGdVmCmdWlOLCksZKKkgRmRO4L8OuPbWdGTSlr1u3moU0HAChNxtj4qdXEtS7WpHLPhj1864lX+PFfXMGQO68c7ObcmdVFq0+xpo82A/NytucCe3ILuHtrzubXgc+HWB85DsOD0ZfMqx23nLuTzjipxNgv5OEWyTM7DlGajLMypwvjU9degAMHO/vZ0NxOe2+au9Y2sy+4L8Tzu9tHWg6FmIE7TK8qYXp1tqVSVZKkqjRBZWmCqtIk1aXZ1lNVaZLa8iTJeIxUIsb0qmz5rv5BasqSlCbHb330DmRoPtzD4sbKMYsQDg35qH39gxlu+k4T21u6WfOhK6krT52yhQv70hk+c++mke23XjKbX76wj770EF/79Tb+atXionez9Q9maO9Nj3QNRtnwcjXP7DjEvc/v5fu/fZUnPvZ73PD1p/jL313MDZdPnt6NMIPgGWCpmZ1FdlbQu4EbcguY2Sx33xtsXgtsQqYUMyOVKPzlM7yo36pzpo85VlWavSK7ujTJosZKAG583cIx5Q509JFxp60nzd6gm+lQ9wCHugcwM/a199LS2U97b5qWzn46+wbp6hukM2/K7nhKEjFqy5PUlCWpLUtRU56ktixJZWmCna09PLmtld50hlXnNPKu5fN4fnc7m/Z2cLgnzbpdbVyxqJ53XjaXRCzGYy+18PjLBwF47T89BMCS6ZVcPLeWpTMqufrc6SyZXklmyMfM9Boacrbs72Ttq4e5f+N+Vl8wk3ddNo9bvv8sQ+78yaVzR5W/6tzpfOZtr+GSTz3AF+7fwgMb95GIx7hhxXxWLq5nTm3ZhP8NTgV354M/eI5Hthzgex9YwesWN9A7kKE0mT3Pz/1yMxfOruGPLp59WutVDPva+0ZeP7L5AA8HrbfvPbmTVw/1cP/GfZMqCMKePnoNcDvZ6aPfcvfPmNmngSZ3X2Nm/0w2AAaBQ8At7r55vPdU15BMxNCQ0zWQDYWOvjRtPWkGM05fOsOBzn4O9wxQXZaksy9Ne0/2eFvvAG09adp7s4+O3jSza8tYcdY0kvEY331yB0MOiZixuLGSslScC2ZX88sX9o2aAvzWS2aTjMf4vw176U1nqCpNjBonqUjFSQ85ixoqqClLcv7sarr6Bnl+d/vIgPqw82dV8+LejpHtqpLESMg99tHfY359OS/sbuef/u9FWrsG6BnIsLutl9JkjEvn1zGzupSL59XyhrMbmVFdQlkyHlqr4f6N+/iL7z0LwGsX1PHVP72UP7zjCS6cXc0fL5vDrf+9DoB/e8fFfOmhl1jzodczrSIVSl2K7e7nmvnbH68nFY9x2Vl1vLgn+4fDsIpUnPWf/P3TOu1bS0yInAIdfWm2t3SzZHollSVHGtMDg0P88KmdnD+7hkWNFTRUlgDZmT196QylyTjbW7r49UstdPYN0t6bpi+d7TI7EIyhVJQkmF1byh9cMJPXL21gcWMlq29/jB2tPdxw+XzW7jzM5n2dfPn6ZXzuvs3sbuvllX++ZsyXeu9Ahgde3BesldVB/2CGdObI/+PTKlIsqC+nviJFdWmSuooUqUSMIXfKknFm15ZRW5bdP60iRXkqTnkyO3hfqPtv2JPbWvngD9cyvaqEP142h8/dt5kLZlezcU/HmLKpeIyBzBDvvmwe63a18Z6VC1ixcBodfWkuWzjtpP4bHcuBjj5auwdYMr0y1PGlj/50PQ+8uJ/fPbuRNev3FCwzt66MxqoSbr16KemM86bzZ4RWH1AQiExJ+zv6ONwzwLkzq9nd1svh7gEunFPDoe4B2nvTE5o5trO1m417OtjR2k1/eoh97X00t/VwuDvb6jncM0A6M4RhpIeGGO/rIBWPUZqMkYjHiJmRiBnxmJEZcvZ19DG7ppTv33Q59RUlXPzpBwB478oF/GLdbjr6BvnL313Mz55t5mBXP+fOrBrT+gF427I5tHYPsHLRNM6fVU1feoizZ1Syv6Mfx7l0ft0xx3SGDY9RHeoeYGdrD3c/t5u7nmvGHRY1VPCNG5ezqLGStp4BPnvvJu7fuJ8LZlfzL9ddxNy6I1O5t+zLdtctqC+f0Of3DmRY8dmHuPrc6ZzVUMmXHnoJgNctruc321rHtPKGPfOPb6SxqmRC53YitNaQyBQ0o7qUGdXZQdc5tWUjff7Tgr/WJ2JBfQUL6o8dGJBt2ext7+VwT5pD3dnxlp6BDL0DGXrTGTr60vSnh8gMOYNDzlDwDHDR3Breddm8kS/JT/7R+WzZ18nf//45XL5oGhua2/nQVUtYOr2S7Qe7uPbiOdz03Wf48yvP4huPv0JpMkZ9RQl3P7ebObVlPPZSy1HrmYgZibhRloxTVZoM6jPE8CUzBjjQ0ZumP+dGU8m48edXnsU5M6v4/H2beed/PslV507nvuf30ZPOcM1rZvHo5gO8+fbHeddl8zCDx18+OCqwUokYl86v5XWLG7hobg0NlSXZm1sFdfLg3uedfYNcv2I++zuPzJpbdU4jv9nWyrmzqgoGwWWfeYiZ1aW8Y/lcOvsGufysaexpzwbsa+bW8NVHt/GWi2ZzxeJTf92IWgQiUlR9wdXyJYkYrd0D1Fek2LS3k9buflLxGDsP9TC7poy+dIYX9rSTzgwxmHG6gzGgRDxGIjZ8hfyR77Oq0uRIaM6oLuW1C+pGuvRe3t/Jx+96npf2d/L6JQ3c+salnDuzmp2t3Xzuvs3cv3EfiViM1y6o4+rzpnP1eTPY3tLFk9ta+c22Vjbt6xi39XTD5fP57NteQ2tXP7f9YiOvmVvDG5Y2cs0dj/ODmy6nPBUnGSxeebgnzSsHu/hJU/OY8aR8H1t9LresWnxC/87qGhIROQ5d/YMk43bUZVAOdw+wtaUrGKAfHGkhDXl2+ZZL59cWHJQfzAwVHCDODDmtXf1UlCRY++phKksS3LV2NyWJGMvm17G9pYu6ihTvuXz+CQ/2q2tIROQ45E4GKKSuIsVlFcc/sH20WULxmDE96Ab8naWNACybX3fc73+ionVZpoiIjKEgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiptyVxWbWAuw8wR9vAA6ewupMBTrnaNA5R8PJnPMCd28sdGDKBcHJMLOmo11ifabSOUeDzjkawjpndQ2JiEScgkBEJOKiFgR3FrsCRaBzjgadczSEcs6RGiMQEZGxotYiEBGRPAoCEZGIi0wQmNlqM9tiZlvN7OPFrs+pYmbfMrMDZvZCzr5pZvagmb0cPNcF+83M7gj+DTaY2aXFq/mJM7N5ZvaImW0ys41mdmuw/4w9bzMrNbOnzWx9cM6fCvafZWZPBef8YzNLBftLgu2twfGFxaz/iTKzuJk9Z2b3BNtn9PkCmNkOM3vezNaZWVOwL9Tf7UgEgZnFgf8A3gycD1xvZucXt1anzLeB1Xn7Pg487O5LgYeDbcie/9LgcTPw1dNUx1NtEPiIu58HrAQ+GPz3PJPPux+4yt0vBi4BVpvZSuDzwJeCcz4MfCAo/wHgsLsvAb4UlJuKbgU25Wyf6ec77Pfc/ZKcawbC/d129zP+AVwB3J+z/QngE8Wu1yk8v4XACznbW4BZwetZwJbg9X8C1xcqN5UfwC+AN0XlvIFyYC1wOdmrTBPB/pHfc+B+4IrgdSIoZ8Wu+3Ge59zgS+8q4B7AzuTzzTnvHUBD3r5Qf7cj0SIA5gC7crabg31nqhnuvhcgeJ4e7D/j/h2CLoBlwFOc4ecddJOsAw4ADwLbgDZ3HwyK5J7XyDkHx9uB+tNb45N2O/D/gKFgu54z+3yHOfCAmT1rZjcH+0L93Y7KzeutwL4ozps9o/4dzKwS+DnwN+7eYVbo9LJFC+ybcuft7hngEjOrBe4GzitULHie0udsZm8BDrj7s2a2anh3gaJnxPnmudLd95jZdOBBM9s8TtlTct5RaRE0A/NytucCe4pUl9Nhv5nNAgieDwT7z5h/BzNLkg2BH7j7XcHuM/68Ady9DXiU7PhIrZkN/0GXe14j5xwcrwEOnd6anpQrgWvNbAfw32S7h27nzD3fEe6+J3g+QDbwVxDy73ZUguAZYGkw4yAFvBtYU+Q6hWkNcGPw+kayfejD+98XzDRYCbQPNzenEsv+6f9NYJO7fzHn0Bl73mbWGLQEMLMy4I1kB1EfAbaXN8sAAAJgSURBVK4LiuWf8/C/xXXArzzoRJ4K3P0T7j7X3ReS/f/1V+7+Hs7Q8x1mZhVmVjX8Gvh94AXC/t0u9sDIaRyAuQZ4iWy/6j8Wuz6n8Lx+BOwF0mT/OvgA2b7Rh4GXg+dpQVkjO3tqG/A8sLzY9T/Bc3492ebvBmBd8LjmTD5v4CLgueCcXwBuC/YvAp4GtgI/BUqC/aXB9tbg+KJin8NJnPsq4J4onG9wfuuDx8bh76qwf7e1xISISMRFpWtIRESOQkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEInnMLBOs/Dj8OGWr1ZrZQstZKVZkMojKEhMix6PX3S8pdiVEThe1CEQmKFgn/vPBfQGeNrMlwf4FZvZwsB78w2Y2P9g/w8zuDu4hsN7MXhe8VdzMvh7cV+CB4EphkaJREIiMVZbXNfSunGMd7r4C+ArZtW8IXn/X3S8CfgDcEey/A/i1Z+8hcCnZK0Uhu3b8f7j7BUAb8PaQz0dkXLqyWCSPmXW5e2WB/TvI3hxme7Do3T53rzezg2TXgE8H+/e6e4OZtQBz3b0/5z0WAg969gYjmNnHgKS7/1P4ZyZSmFoEIsfHj/L6aGUK6c95nUFjdVJkCgKR4/OunOcng9e/IbtCJsB7gCeC1w8Dt8DITWWqT1clRY6H/hIRGassuBPYsF+6+/AU0hIze4rsH1HXB/s+DHzLzD4KtADvD/bfCtxpZh8g+5f/LWRXihWZVDRGIDJBwRjBcnc/WOy6iJxK6hoSEYk4tQhERCJOLQIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYm4/w+MBfcV17mhKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs),final_losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## prediction in the x_test data\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_test):\n",
    "        y_pred = model(data)\n",
    "        predictions.append(y_pred.argmax().item())\n",
    "        print(y_pred.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[92, 15],\n",
       "       [17, 30]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(69.0, 0.5, 'Predicted values')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.xlabel(\"Actual value\")\n",
    "plt.ylabel(\"Predicted values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7922077922077922"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_test,predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n",
    "torch.save(model,\"diabetes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "model=torch.load(\"diabetes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN_Model(\n",
       "  (f_connected1): Linear(in_features=8, out_features=20, bias=True)\n",
       "  (f_connected2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (out): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction of new data\n",
    "list = [6.0, 130.0, 73.0, 48.0, 0.0, 35.6, 0.627, 45.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = torch.tensor(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1727, 1.3417])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## predict new data using pytorch \n",
    "with torch.no_grad():\n",
    "    print(model(new_data))\n",
    "    print(model(new_data).argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
